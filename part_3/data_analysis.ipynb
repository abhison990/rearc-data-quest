{
 "cells": [
  {
   "cell_type": "code",
   "id": "19e5a27c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:00:20.880398Z",
     "start_time": "2025-03-17T19:00:16.777493Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, sum, trim, max_by, row_number\n",
    "from pyspark.sql import SparkSession, Window\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, sum\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Rearc Data Analysis\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set up Hadoop AWS configurations\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "\n",
    "print(\"SPARK SESSION CREATED SUCCESSFULLY\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/18 00:30:19 WARN Utils: Your hostname, Abhijeets-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "25/03/18 00:30:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/abhijeet/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/abhijeet/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87e33019-14ea-4a64-b5b9-9f0c208bb336;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/abhijeet/Desktop/Mine/abhijeet/rearc-data-quest/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 121ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87e33019-14ea-4a64-b5b9-9f0c208bb336\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/03/18 00:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK SESSION CREATED SUCCESSFULLY\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6da2320d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:07:36.600978Z",
     "start_time": "2025-03-17T19:07:32.193236Z"
    }
   },
   "source": [
    "# AWS S3 Configuration\n",
    "S3_BUCKET = \"abh-de-rearc\"\n",
    "CSV_S3_PATH = f\"s3a://{S3_BUCKET}/bls-data/pr.data.0.Current\"\n",
    "JSON_S3_PATH = f\"s3a://{S3_BUCKET}/api-data/us_population.json\"\n",
    "\n",
    "# Load CSV file from S3 into PySpark DataFrame\n",
    "print(\"Loading CSV data from S3...\")\n",
    "csv_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(CSV_S3_PATH)\n",
    "\n",
    "# Rename columns properly\n",
    "csv_df = csv_df.toDF(\"series_id\", \"year\", \"period\", \"value\", \"footnote_codes\")\n",
    "\n",
    "# Trim strings to remove unwanted spaces\n",
    "csv_df = csv_df.withColumn(\"series_id\", trim(col(\"series_id\"))) \\\n",
    "               .withColumn(\"period\", trim(col(\"period\"))) \\\n",
    "               .withColumn(\"value\", col(\"value\").cast(\"float\"))  # Ensure value is float\n",
    "\n",
    "# Load JSON file from S3 into PySpark DataFrame\n",
    "print(\"Loading JSON data from S3...\")\n",
    "json_df = spark.read.option(\"multiline\", \"true\").json(JSON_S3_PATH)\n",
    "\n",
    "# Extract relevant columns from JSON\n",
    "population_df = json_df.selectExpr(\"explode(data) as data\").select(\n",
    "    col(\"data.Year\").alias(\"year\"),\n",
    "    col(\"data.Population\").alias(\"population\")\n",
    ")\n",
    "\n",
    "# Convert population to integer\n",
    "population_df = population_df.withColumn(\"population\", col(\"population\").cast(\"int\"))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV data from S3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/18 00:37:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/18 00:37:32 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON data from S3...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:07:37.085796Z",
     "start_time": "2025-03-17T19:07:36.609723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Task 1: Calculate Mean & Std Dev of Population (2013-2018) ---\n",
    "pop_stats_df = population_df.filter((col(\"year\") >= 2013) & (col(\"year\") <= 2018))\n",
    "pop_summary_df = pop_stats_df.select(mean(\"population\").alias(\"mean_population\"),\n",
    "                                     stddev(\"population\").alias(\"stddev_population\"))\n",
    "\n",
    "print(\"Population Statistics (2013-2018):\")\n",
    "pop_summary_df.show()"
   ],
   "id": "a56b1d1e30ba695f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population Statistics (2013-2018):\n",
      "+---------------+-----------------+\n",
      "|mean_population|stddev_population|\n",
      "+---------------+-----------------+\n",
      "|   3.17437383E8| 4257089.54152933|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:09:41.431835Z",
     "start_time": "2025-03-17T19:09:39.980035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Task 2: Find Best Year for Each `series_id` (Max Sum of `value`) ---\n",
    "agg_df = csv_df.groupBy(\"series_id\", \"year\").agg(sum(\"value\").alias(\"total_value\"))\n",
    "\n",
    "# Define a window specification to rank years by total_value per series_id\n",
    "window_spec = Window.partitionBy(\"series_id\").orderBy(col(\"total_value\").desc())\n",
    "\n",
    "# Assign ranks to each year and keep only the best (highest sum) year for each series_id\n",
    "best_year_df = agg_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Order the final results by `year` before displaying\n",
    "best_year_df = best_year_df.orderBy(\"year\")\n",
    "\n",
    "print(\"Best Year for Each Series ID:\")\n",
    "best_year_df.show(50, truncate=False)\n",
    "\n"
   ],
   "id": "502568ebcdc2b2d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Year for Each Series ID:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------------+\n",
      "|series_id  |year|total_value       |\n",
      "+-----------+----+------------------+\n",
      "|PRS32006013|1995|726.1410064697266 |\n",
      "|PRS32006033|1995|710.8509826660156 |\n",
      "|PRS30006212|1997|38.80000019073486 |\n",
      "|PRS31006023|1997|503.9239959716797 |\n",
      "|PRS31006211|1997|47.60000038146973 |\n",
      "|PRS31006212|1997|55.5              |\n",
      "|PRS84006023|1997|518.505989074707  |\n",
      "|PRS85006023|1997|519.088996887207  |\n",
      "|PRS88003023|1997|517.8190002441406 |\n",
      "|PRS31006033|1998|705.6860046386719 |\n",
      "|PRS30006013|1998|705.8950042724609 |\n",
      "|PRS31006013|1998|707.8460083007812 |\n",
      "|PRS30006033|1998|702.6719970703125 |\n",
      "|PRS32006173|1998|137.18699645996094|\n",
      "|PRS31006101|2000|34.59999942779541 |\n",
      "|PRS30006173|2001|123.10700225830078|\n",
      "|PRS31006173|2001|112.14299774169922|\n",
      "|PRS84006173|2001|559.6599960327148 |\n",
      "|PRS85006173|2001|558.5790023803711 |\n",
      "|PRS88003173|2001|558.625           |\n",
      "|PRS30006091|2002|43.299999713897705|\n",
      "|PRS30006092|2002|44.39999961853027 |\n",
      "|PRS30006162|2002|48.100000858306885|\n",
      "|PRS32006091|2002|35.0              |\n",
      "|PRS31006092|2002|49.80000019073486 |\n",
      "|PRS31006162|2002|54.69999933242798 |\n",
      "|PRS32006161|2002|36.0              |\n",
      "|PRS32006162|2002|31.59999990463257 |\n",
      "|PRS88003182|2002|308.0             |\n",
      "|PRS88003192|2002|282.7999954223633 |\n",
      "|PRS32006102|2003|34.60000038146973 |\n",
      "|PRS32006092|2004|31.700000286102295|\n",
      "|PRS30006213|2007|523.8250045776367 |\n",
      "|PRS31006213|2007|523.3639984130859 |\n",
      "|PRS32006213|2007|517.0149993896484 |\n",
      "|PRS30006112|2008|42.69999933242798 |\n",
      "|PRS31006112|2008|58.29999981075525 |\n",
      "|PRS32006112|2008|34.90000081062317 |\n",
      "|PRS31006111|2009|37.49999952316284 |\n",
      "|PRS31006171|2009|8.899999618530273 |\n",
      "|PRS31006172|2009|8.899999618530273 |\n",
      "|PRS84006092|2009|30.5              |\n",
      "|PRS85006092|2009|30.09999990463257 |\n",
      "|PRS30006161|2010|50.59999990463257 |\n",
      "|PRS30006211|2010|35.0              |\n",
      "|PRS31006021|2010|19.399999856948853|\n",
      "|PRS31006022|2010|14.299999952316284|\n",
      "|PRS30006021|2010|17.699999570846558|\n",
      "|PRS31006091|2010|42.80000019073486 |\n",
      "|PRS31006161|2010|64.10000133514404 |\n",
      "+-----------+----+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T19:12:42.703474Z",
     "start_time": "2025-03-17T19:12:42.164934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Task 3: Report for `series_id = PRS30006032` and `period = Q01` ---\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure \"year\" is integer in both DataFrames\n",
    "csv_df = csv_df.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "population_df = population_df.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "\n",
    "\n",
    "# Ensure we **retain all years** from the CSV (using full outer join for debugging)\n",
    "final_report_df = (\n",
    "    csv_df\n",
    "    .filter(col(\"series_id\") == \"PRS30006032\")  # Filter for the specific series\n",
    "    .filter(col(\"period\") == \"Q01\")\n",
    "    .join(population_df, on=\"year\", how=\"inner\")  # inner join retains all CSV years\n",
    "    .select(\"series_id\", \"year\", \"period\", \"value\", \"population\")  # Clean output\n",
    ")\n",
    "\n",
    "#DEBUG STEP: Show the results before saving\n",
    "final_report_df.orderBy(\"year\").show()  # Ensure all years are present\n"
   ],
   "id": "2356ef8a823c33f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+----------+\n",
      "|  series_id|year|period|value|population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|2013|   Q01|  0.5| 311536594|\n",
      "|PRS30006032|2014|   Q01| -0.1| 314107084|\n",
      "|PRS30006032|2015|   Q01| -1.7| 316515021|\n",
      "|PRS30006032|2016|   Q01| -1.4| 318558162|\n",
      "|PRS30006032|2017|   Q01|  0.9| 321004407|\n",
      "|PRS30006032|2018|   Q01|  0.5| 322903030|\n",
      "|PRS30006032|2019|   Q01| -1.6| 324697795|\n",
      "|PRS30006032|2020|   Q01| -7.0| 326569308|\n",
      "|PRS30006032|2021|   Q01|  0.7| 329725481|\n",
      "|PRS30006032|2022|   Q01|  5.2| 331097593|\n",
      "+-----------+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
