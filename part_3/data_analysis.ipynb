{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"19e5a27c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"25/03/14 17:16:58 WARN Utils: Your hostname, Abhijeets-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.7 instead (on interface en0)\\n\",\n",
    "      \"25/03/14 17:16:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\\n\",\n",
    "      \"Setting default log level to \\\"WARN\\\".\\n\",\n",
    "      \"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\\n\",\n",
    "      \"25/03/14 17:16:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"SPARK SESSION CREATED SUCCESSFULLY\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"from pyspark.sql.functions import col, mean, stddev, sum, trim, row_number\\n\",\n",
    "    \"from pyspark.sql import SparkSession, Window\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize Spark Session with the same working configurations\\n\",\n",
    "    \"spark = SparkSession.builder \\\\\\n\",\n",
    "    \"    .appName(\\\"Rearc Data Analysis\\\") \\\\\\n\",\n",
    "    \"    .master(\\\"local[2]\\\") \\\\\\n\",\n",
    "    \"    .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\\\\\n\",\n",
    "    \"    .getOrCreate()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up S3 credentials and Hadoop AWS configurations\\n\",\n",
    "    \"hadoop_conf = spark._jsc.hadoopConfiguration()\\n\",\n",
    "    \"hadoop_conf.set(\\\"fs.s3a.endpoint\\\", \\\"s3.amazonaws.com\\\")\\n\",\n",
    "    \"hadoop_conf.set(\\\"fs.s3a.impl\\\", \\\"org.apache.hadoop.fs.s3a.S3AFileSystem\\\")\\n\",\n",
    "    \"hadoop_conf.set(\\\"com.amazonaws.services.s3.enableV4\\\", \\\"true\\\")\\n\",\n",
    "    \"hadoop_conf.set(\\\"fs.s3a.aws.credentials.provider\\\",\\n\",\n",
    "    \"                \\\"com.amazonaws.auth.InstanceProfileCredentialsProvider,com.amazonaws.auth.DefaultAWSCredentialsProviderChain\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"SPARK SESSION CREATED SUCCESSFULLY\\\")\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f08d1780\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"745f77fa\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Loading CSV data from S3...\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"25/03/14 17:17:02 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://abh-de-rearc/bls-data/pr.data.0.Current.\\n\",\n",
    "      \"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\",\n",
    "      \"\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\\n\",\n",
    "      \"\\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\\n\",\n",
    "      \"\\tat scala.Option.getOrElse(Option.scala:189)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\\n\",\n",
    "      \"\\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\\n\",\n",
    "      \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\",\n",
    "      \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\",\n",
    "      \"\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\",\n",
    "      \"\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\",\n",
    "      \"\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\",\n",
    "      \"\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\",\n",
    "      \"\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\",\n",
    "      \"\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\",\n",
    "      \"\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\",\n",
    "      \"\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\",\n",
    "      \"\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\",\n",
    "      \"\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\",\n",
    "      \"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\",\n",
    "      \"\\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\\n\",\n",
    "      \"\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\\n\",\n",
    "      \"\\t... 26 more\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"ename\": \"Py4JJavaError\",\n",
    "     \"evalue\": \"An error occurred while calling o38.csv.\\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\\n\\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\\n\\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\\n\\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\\n\\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\\n\\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\\n\\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\\n\\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\\n\\tat scala.collection.immutable.List.map(List.scala:293)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\\n\\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\\n\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\\n\\t... 29 more\\n\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
    "      \"\\u001b[0;31mPy4JJavaError\\u001b[0m                             Traceback (most recent call last)\",\n",
    "      \"Cell \\u001b[0;32mIn[3], line 8\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[38;5;66;03m# Load CSV file from S3 into PySpark DataFrame\\u001b[39;00m\\n\\u001b[1;32m      7\\u001b[0m \\u001b[38;5;28mprint\\u001b[39m(\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mLoading CSV data from S3...\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m)\\n\\u001b[0;32m----> 8\\u001b[0m csv_df \\u001b[38;5;241m=\\u001b[39m \\u001b[43mspark\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mread\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43moption\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mheader\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mfalse\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m)\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43moption\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mdelimiter\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;130;43;01m\\\\t\\u001b[39;49;00m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m)\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mcsv\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mCSV_S3_PATH\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m     10\\u001b[0m \\u001b[38;5;66;03m# Rename columns properly\\u001b[39;00m\\n\\u001b[1;32m     11\\u001b[0m csv_df \\u001b[38;5;241m=\\u001b[39m csv_df\\u001b[38;5;241m.\\u001b[39mtoDF(\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mseries_id\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124myear\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mperiod\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mvalue\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mfootnote_codes\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m)\\n\",\n",
    "      \"File \\u001b[0;32m/Library/Python/3.9/site-packages/pyspark/sql/readwriter.py:740\\u001b[0m, in \\u001b[0;36mDataFrameReader.csv\\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\\u001b[0m\\n\\u001b[1;32m    738\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;28mtype\\u001b[39m(path) \\u001b[38;5;241m==\\u001b[39m \\u001b[38;5;28mlist\\u001b[39m:\\n\\u001b[1;32m    739\\u001b[0m     \\u001b[38;5;28;01massert\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_spark\\u001b[38;5;241m.\\u001b[39m_sc\\u001b[38;5;241m.\\u001b[39m_jvm \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m\\n\\u001b[0;32m--> 740\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_df(\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_jreader\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mcsv\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_spark\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_sc\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_jvm\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mPythonUtils\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mtoSeq\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mpath\\u001b[49m\\u001b[43m)\\u001b[49m\\u001b[43m)\\u001b[49m)\\n\\u001b[1;32m    741\\u001b[0m \\u001b[38;5;28;01melif\\u001b[39;00m \\u001b[38;5;28misinstance\\u001b[39m(path, RDD):\\n\\u001b[1;32m    743\\u001b[0m     \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21mfunc\\u001b[39m(iterator):\\n\",\n",
    "      \"File \\u001b[0;32m/Library/Python/3.9/site-packages/py4j/java_gateway.py:1322\\u001b[0m, in \\u001b[0;36mJavaMember.__call__\\u001b[0;34m(self, *args)\\u001b[0m\\n\\u001b[1;32m   1316\\u001b[0m command \\u001b[38;5;241m=\\u001b[39m proto\\u001b[38;5;241m.\\u001b[39mCALL_COMMAND_NAME \\u001b[38;5;241m+\\u001b[39m\\\\\\n\\u001b[1;32m   1317\\u001b[0m     \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mcommand_header \\u001b[38;5;241m+\\u001b[39m\\\\\\n\\u001b[1;32m   1318\\u001b[0m     args_command \\u001b[38;5;241m+\\u001b[39m\\\\\\n\\u001b[1;32m   1319\\u001b[0m     proto\\u001b[38;5;241m.\\u001b[39mEND_COMMAND_PART\\n\\u001b[1;32m   1321\\u001b[0m answer \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mgateway_client\\u001b[38;5;241m.\\u001b[39msend_command(command)\\n\\u001b[0;32m-> 1322\\u001b[0m return_value \\u001b[38;5;241m=\\u001b[39m \\u001b[43mget_return_value\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[1;32m   1323\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43manswer\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mgateway_client\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mtarget_id\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mname\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m   1325\\u001b[0m \\u001b[38;5;28;01mfor\\u001b[39;00m temp_arg \\u001b[38;5;129;01min\\u001b[39;00m temp_args:\\n\\u001b[1;32m   1326\\u001b[0m     \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;28mhasattr\\u001b[39m(temp_arg, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m_detach\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m):\\n\",\n",
    "      \"File \\u001b[0;32m/Library/Python/3.9/site-packages/pyspark/errors/exceptions/captured.py:179\\u001b[0m, in \\u001b[0;36mcapture_sql_exception.<locals>.deco\\u001b[0;34m(*a, **kw)\\u001b[0m\\n\\u001b[1;32m    177\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21mdeco\\u001b[39m(\\u001b[38;5;241m*\\u001b[39ma: Any, \\u001b[38;5;241m*\\u001b[39m\\u001b[38;5;241m*\\u001b[39mkw: Any) \\u001b[38;5;241m-\\u001b[39m\\u001b[38;5;241m>\\u001b[39m Any:\\n\\u001b[1;32m    178\\u001b[0m     \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[0;32m--> 179\\u001b[0m         \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mf\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43ma\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkw\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m    180\\u001b[0m     \\u001b[38;5;28;01mexcept\\u001b[39;00m Py4JJavaError \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\\u001b[1;32m    181\\u001b[0m         converted \\u001b[38;5;241m=\\u001b[39m convert_exception(e\\u001b[38;5;241m.\\u001b[39mjava_exception)\\n\",\n",
    "      \"File \\u001b[0;32m/Library/Python/3.9/site-packages/py4j/protocol.py:326\\u001b[0m, in \\u001b[0;36mget_return_value\\u001b[0;34m(answer, gateway_client, target_id, name)\\u001b[0m\\n\\u001b[1;32m    324\\u001b[0m value \\u001b[38;5;241m=\\u001b[39m OUTPUT_CONVERTER[\\u001b[38;5;28mtype\\u001b[39m](answer[\\u001b[38;5;241m2\\u001b[39m:], gateway_client)\\n\\u001b[1;32m    325\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m answer[\\u001b[38;5;241m1\\u001b[39m] \\u001b[38;5;241m==\\u001b[39m REFERENCE_TYPE:\\n\\u001b[0;32m--> 326\\u001b[0m     \\u001b[38;5;28;01mraise\\u001b[39;00m Py4JJavaError(\\n\\u001b[1;32m    327\\u001b[0m         \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mAn error occurred while calling \\u001b[39m\\u001b[38;5;132;01m{0}\\u001b[39;00m\\u001b[38;5;132;01m{1}\\u001b[39;00m\\u001b[38;5;132;01m{2}\\u001b[39;00m\\u001b[38;5;124m.\\u001b[39m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\n\\u001b[1;32m    328\\u001b[0m         \\u001b[38;5;28mformat\\u001b[39m(target_id, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, name), value)\\n\\u001b[1;32m    329\\u001b[0m \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[1;32m    330\\u001b[0m     \\u001b[38;5;28;01mraise\\u001b[39;00m Py4JError(\\n\\u001b[1;32m    331\\u001b[0m         \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mAn error occurred while calling \\u001b[39m\\u001b[38;5;132;01m{0}\\u001b[39;00m\\u001b[38;5;132;01m{1}\\u001b[39;00m\\u001b[38;5;132;01m{2}\\u001b[39;00m\\u001b[38;5;124m. Trace:\\u001b[39m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;132;01m{3}\\u001b[39;00m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\n\\u001b[1;32m    332\\u001b[0m         \\u001b[38;5;28mformat\\u001b[39m(target_id, \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, name, value))\\n\",\n",
    "      \"\\u001b[0;31mPy4JJavaError\\u001b[0m: An error occurred while calling o38.csv.\\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\\n\\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\\n\\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\\n\\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\\n\\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\\n\\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\\n\\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\\n\\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\\n\\tat scala.collection.immutable.List.map(List.scala:293)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\\n\\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\\n\\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\\n\\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\\n\\t... 29 more\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"25/03/14 17:17:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# AWS S3 Configuration\\n\",\n",
    "    \"S3_BUCKET = \\\"abh-de-rearc\\\"\\n\",\n",
    "    \"CSV_S3_PATH = f\\\"s3a://{S3_BUCKET}/bls-data/pr.data.0.Current\\\"\\n\",\n",
    "    \"JSON_S3_PATH = f\\\"s3a://{S3_BUCKET}/api-data/us_population.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load CSV file from S3 into PySpark DataFrame\\n\",\n",
    "    \"print(\\\"Loading CSV data from S3...\\\")\\n\",\n",
    "    \"csv_df = spark.read.option(\\\"header\\\", \\\"false\\\").option(\\\"delimiter\\\", \\\"\\\\t\\\").csv(CSV_S3_PATH)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Rename columns properly\\n\",\n",
    "    \"csv_df = csv_df.toDF(\\\"series_id\\\", \\\"year\\\", \\\"period\\\", \\\"value\\\", \\\"footnote_codes\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Trim strings to remove unwanted spaces\\n\",\n",
    "    \"csv_df = csv_df.withColumn(\\\"series_id\\\", trim(col(\\\"series_id\\\"))) \\\\\\n\",\n",
    "    \"               .withColumn(\\\"period\\\", trim(col(\\\"period\\\"))) \\\\\\n\",\n",
    "    \"               .withColumn(\\\"value\\\", col(\\\"value\\\").cast(\\\"float\\\"))  # Ensure value is float\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load JSON file from S3 into PySpark DataFrame\\n\",\n",
    "    \"print(\\\"Loading JSON data from S3...\\\")\\n\",\n",
    "    \"json_df = spark.read.option(\\\"multiline\\\", \\\"true\\\").json(JSON_S3_PATH)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extract relevant columns from JSON\\n\",\n",
    "    \"population_df = json_df.selectExpr(\\\"explode(data) as data\\\").select(\\n\",\n",
    "    \"    col(\\\"data.Year\\\").alias(\\\"year\\\"),\\n\",\n",
    "    \"    col(\\\"data.Population\\\").alias(\\\"population\\\")\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert population to integer\\n\",\n",
    "    \"population_df = population_df.withColumn(\\\"population\\\", col(\\\"population\\\").cast(\\\"int\\\"))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"742843dd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9d980010\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9ca9dd03\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# --- Task 1: Calculate Mean & Std Dev of Population (2013-2018) ---\\n\",\n",
    "    \"pop_stats_df = population_df.filter((col(\\\"year\\\") >= 2013) & (col(\\\"year\\\") <= 2018))\\n\",\n",
    "    \"pop_summary_df = pop_stats_df.select(mean(\\\"population\\\").alias(\\\"mean_population\\\"),\\n\",\n",
    "    \"                                     stddev(\\\"population\\\").alias(\\\"stddev_population\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Population Statistics (2013-2018):\\\")\\n\",\n",
    "    \"pop_summary_df.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"e23373f7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# --- Task 2: Find Best Year for Each `series_id` (Max Sum of `value`) ---\\n\",\n",
    "    \"agg_df = csv_df.groupBy(\\\"series_id\\\", \\\"year\\\").agg(sum(\\\"value\\\").alias(\\\"total_value\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define a window specification to rank years by total_value per series_id\\n\",\n",
    "    \"window_spec = Window.partitionBy(\\\"series_id\\\").orderBy(col(\\\"total_value\\\").desc())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Assign ranks to each year and keep only the best (highest sum) year for each series_id\\n\",\n",
    "    \"best_year_df = agg_df.withColumn(\\\"rank\\\", row_number().over(window_spec)).filter(col(\\\"rank\\\") == 1).drop(\\\"rank\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Best Year for Each Series ID:\\\")\\n\",\n",
    "    \"best_year_df.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"0cbacd9c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# --- Task 3: Report for `series_id = PRS30006032` and `period = Q01` ---\\n\",\n",
    "    \"filtered_df = csv_df.filter((col(\\\"series_id\\\") == \\\"PRS30006032\\\") & (col(\\\"period\\\") == \\\"Q01\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Join with population data\\n\",\n",
    "    \"final_report_df = filtered_df.join(population_df, \\\"year\\\", \\\"left\\\").select(\\n\",\n",
    "    \"    \\\"series_id\\\", \\\"year\\\", \\\"period\\\", \\\"value\\\", \\\"population\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Final Report for `PRS30006032`, `Q01`:\\\")\\n\",\n",
    "    \"final_report_df.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"92ea3a3c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# Save all results as CSV in S3\\n\",\n",
    "    \"output_path = f\\\"s3a://{S3_BUCKET}/analytics-results/\\\"\\n\",\n",
    "    \"pop_summary_df.write.mode(\\\"overwrite\\\").csv(output_path + \\\"population_summary\\\")\\n\",\n",
    "    \"best_year_df.write.mode(\\\"overwrite\\\").csv(output_path + \\\"best_year_per_series\\\")\\n\",\n",
    "    \"final_report_df.write.mode(\\\"overwrite\\\").csv(output_path + \\\"final_report\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Results saved to: {output_path}\\\")\\n\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "5714319bdf760c22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
